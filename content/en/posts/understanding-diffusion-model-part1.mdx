---
title: "Understanding Diffusion Models: A Unified Perspective - Part 1"
description: "Understanding Diffusion Models: A Unified Perspective - Part 1"
authors:
  - name: Calvin Luo
    link: https://twitter.com/calvinyluo
date: 2021-07-11
---
import Image from 'next/image'

Source : https://calvinyluo.com/2022/08/26/diffusion-tutorial.html

A PDF version of this blog post, for printability, can be found [here](https://arxiv.org/abs/2208.11970).

## Introduction: Generative Models
Given observed samples $x$ from a distribution of interest, the goal of a **generative model** is to learn to *model* its true data distribution $p(x)$. Once learned, we can *generate* new samples from our approximate model at will. Furthermore, under some formulations, we are able to use the learned model to evaluate the likelihood of observed or sampled data as well.

There are several well-known directions in current literature, that we will only introduce briefly at a high level. Generative Adversarial Networks (GANs) model the sampling procedure of a complex distribution, which is learned in an adversarial manner. Another class of generative models, termed "likelihood-based", seeks to learn a model that assigns a high likelihood to the observed data samples. This includes autoregressive models, normalizing flows, and Variational Autoencoders (VAEs). Another similar approach is energy-based modeling, in which a distribution is learned as an arbitrarily flexible energy function that is then normalized. Score-based generative models are highly related; instead of learning to model the energy function itself, they learn the *score* of the energy-based model as a neural network. In this work, we explore and review diffusion models, which as we will demonstrate, have both likelihood-based and score-based interpretations. We showcase the math behind such models in excruciating detail, with the aim that anyone can follow along and understand what diffusion models are and how they work.

For many modalities, we can think of the data we observe as represented or generated by an associated unseen *latent* variable, which we can denote by random variable $z$. The best intuition for expressing this idea is through Plato's [Allegory of the Cave](https://en.wikipedia.org/wiki/Allegory_of_the_cave). In the allegory, a group of people are chained inside a cave their entire life and can only see the two-dimensional shadows projected onto a wall in front of them, which are generated by unseen three-dimensional objects passed before a fire. To such people, everything they observe is actually determined by higher-dimensional abstract concepts that they can never behold.

Analogously, the objects that we encounter in the actual world may also be generated as a function of some higher-level representations; for example, such representations may encapsulate abstract properties such as color, size, shape, and more. Then, what we observe can be interpreted as a three-dimensional projection or instantiation of such abstract concepts, just as what the cave people observe is actually a two-dimensional projection of three-dimensional objects. Whereas the cave people can never see (or even fully comprehend) the hidden objects, they can still reason and draw inferences about them; in a similar way, we can approximate latent representations that describe the data we observe.

Whereas Plato's Allegory illustrates the idea behind latent variables as potentially unobservable representations that determine observations, a caveat of this analogy is that in generative modeling, we generally seek to learn lower-dimensional latent representations rather than higher-dimensional ones. This is because trying to learn a representation of higher dimension than the observation is a fruitless endeavor without strong priors. On the other hand, learning lower-dimensional latents can also be seen as a form of compression, and can potentially uncover semantically meaningful structure describing observations.


## Background: ELBO, VAE, and Hierarchical VAE

For many modalities, we can think of the data we observe as represented or generated by an associated unseen *latent* variable, which we can denote by random variable $z$. The best intuition for expressing this idea is through Plato's [Allegory of the Cave](https://en.wikipedia.org/wiki/Allegory_of_the_cave). In the allegory, a group of people are chained inside a cave their entire life and can only see the two-dimensional shadows projected onto a wall in front of them, which are generated by unseen three-dimensional objects passed before a fire. To such people, everything they observe is actually determined by higher-dimensional abstract concepts that they can never behold.

Analogously, the observed data can be generated by latent variables. For example, in the case of an image, the data might be generated by a set of latent variables representing the underlying content or features of the image, which are not directly observable but affect the data we see.

Whereas Plato's Allegory illustrates the idea behind latent variables as potentially unobservable representations that determine observations, a caveat of this analogy is that in generative modeling, we generally seek to learn lower-dimensional latent representations rather than higher-dimensional ones. This is because trying to learn a representation of higher dimension than the observation is a fruitless endeavor without strong priors. On the other hand, learning lower-dimensional latents can also be seen as a form of compression, and can potentially uncover semantically meaningful structure describing observations.


## Evidence Lower Bound

Mathematically, we can imagine the latent variables and the data we observe as modeled by a joint distribution $p(x, z)$.  Recall one approach of generative modeling, termed "likelihood-based", is to learn a model to maximize the likelihood $p(x)$ of all observed $x$.  There are two ways we can manipulate this joint distribution to recover the likelihood of purely our observed data $p(x)$; we can explicitly [marginalize](https://en.wikipedia.org/wiki/Marginal_likelihood) out the latent variable $z$:

$$
\begin{align}
p(x) = \frac{p(x, z)}{p(z|x)}
\end{align}
$$

or, we could also appeal to the [chain rule of probability](https://en.wikipedia.org/wiki/Chain_rule_(probability)):


$$
\begin{align}
\mathbb{E}_{q_{\bm{\phi}}(z|x)}\left[\log\frac{p(x, z)}{q_{\bm{\phi}}(z|x)}\right]
\end{align}
$$

Directly computing and maximizing the likelihood $p(x)$ is difficult because it either involves integrating out all latent variables $z$ in Equation (1), which is intractable for complex models, or it involves having access to a ground truth latent encoder $p(z|x)$ in Equation (2). However, using these two equations, we can derive a term called the **Evidence Lower Bound** (ELBO), which as its name suggests, is a [lower bound](https://en.wikipedia.org/wiki/Upper_and_lower_bounds) of the evidence. The evidence is quantified in this case as the log likelihood of the observed data. Then, maximizing the ELBO becomes a proxy objective with which to optimize a latent variable model; in the best case, when the ELBO is powerfully parameterized and perfectly optimized, it becomes exactly equivalent to the evidence. Formally, the equation of the ELBO is:

$$
\begin{align}
\mathbb{E}_{q_{\bm{\phi}}(z|x)}\left[\log\frac{p(x, z)}{q_{\bm{\phi}}(z|x)}\right]
\end{align}
$$

To make the relationship with the evidence explicit, we can mathematically write:

$$
\begin{align}
\log p(x) \geq \mathbb{E}_{q_{\bm{\phi}}(z|x)}\left[\log\frac{p(x, z)}{q_{\bm{\phi}}(z|x)}\right]
\end{align}
$$

Here, $q_{\bm{\phi}}(z|x)$ is a flexible approximate variational distribution with parameters $\bm{\phi}$ that we seek to optimize.  Intuitively, it can be thought of as a parameterizable model that is learned to estimate the true distribution over latent variables for given observations $x$; in other words, it seeks to approximate true posterior $p(z|x)$.  As we will see when exploring the Variational Autoencoder, as we increase the lower bound by tuning the parameters $\bm{\phi}$ to maximize the ELBO, we gain access to components that can be used to model the true data distribution and sample from it, thus learning a generative model.  For now, let us try to dive deeper into why the ELBO is an objective we would like to maximize.

Let us begin by deriving the ELBO, using Equation (1):

$$
\begin{align}
\log p(x) & = \log \int p(x, z)dz && \text{(Apply Equation)}  \\
           & = \log \int \frac{p(x, z)q_{\bm{\phi}}(z|x)}{q_{\bm{\phi}}(z|x)}dz && \text{(Multiply by $1 = \frac{q_{\bm{\phi}}(z|x)}{q_{\bm{\phi}}(z|x)}$)}  \\
           & = \log \mathbb{E}_{q_{\bm{\phi}}(z|x)}\left[\frac{p(x, z)}{q_{\bm{\phi}}(z|x)}\right] && \text{(Definition of Expectation)}  \\
           & \geq \mathbb{E}_{q_{\bm{\phi}}(z|x)}\left[\log \frac{p(x, z)}{q_{\bm{\phi}}(z|x)}\right]
\end{align}
$$

In this derivation, we directly arrive at our lower bound by applying Jensen's Inequality. However, this does not supply us much useful information about what is actually going on underneath the hood; crucially, this proof gives no intuition on exactly why the ELBO is actually a lower bound of the evidence, as Jensen's Inequality handwaves it away. Furthermore, simply knowing that the ELBO is truly a lower bound of the data does not really tell us why we want to maximize it as an objective. To better understand the relationship between the evidence and the ELBO, let us perform another derivation, this time using Equation 2:


$$
\begin{align}
\log p(\boldsymbol{x}) & = \log p(\boldsymbol{x}) \int q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})dz\\
          & = \int q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})\log p(\boldsymbol{x})dz\\
          & = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log p(\boldsymbol{x})\right]\\
          & = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x}, \boldsymbol{z})}{p(\boldsymbol{z}\mid\boldsymbol{x})}\right]\\
          & = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x}, \boldsymbol{z})q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}{p(\boldsymbol{z}\mid\boldsymbol{x})q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right]\\
          & = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right] + \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}{p(\boldsymbol{z}\mid\boldsymbol{x})}\right]\\
          & = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right] + \mathcal{D}_{\text{KL}} (q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x}) \mid\mid p(\boldsymbol{z}\mid\boldsymbol{x}))\\
          & \geq \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right]
\end{align}
$$

From this derivation, we clearly observe from Equation 15 that the evidence is equal to the ELBO plus the KL Divergence between the approximate posterior $q_{\bm{\phi}}(z|x)$ and the true posterior $p(z|x)$.  In fact, it was this KL Divergence term that was magically removed by Jensen's Inequality in Equation 8 of the first derivation.  Understanding this term is the key to understanding not only the relationship between the ELBO and the evidence, but also the reason why optimizing the ELBO is an appropriate objective at all.

Firstly, we now know why the ELBO is indeed a lower bound: the difference between the evidence and the ELBO is a strictly non-negative KL term, thus the value of the ELBO can never exceed the evidence.

Secondly, we explore why we seek to maximize the ELBO.  Having introduced latent variables $z$ that we would like to model, our goal is to learn this underlying latent structure that describes our observed data.  In other words, we want to optimize the parameters of our variational posterior $q_{\bm{\phi}}(z|x)$ to exactly match the true posterior distribution $p(z|x)$, which is achieved by minimizing their KL Divergence (ideally to zero).  Unfortunately, it is intractable to minimize this KL Divergence term directly, as we do not have access to the ground truth $p(z|x)$ distribution.  However, notice that on the left hand side of Equation 15, the likelihood of our data (and therefore our evidence term $\log p(x)$) is always a constant with respect to $\bm{\phi}$, as it is computed by marginalizing out all latents $z$ from the joint distribution $p(x, z)$ and does not depend on $\bm{\phi}$ whatsoever.  Since the ELBO and KL Divergence terms sum up to a constant, any maximization of the ELBO term with respect to $\bm{\phi}$ necessarily invokes an equal minimization of the KL Divergence term.  Thus, the ELBO can be maximized as a proxy for learning how to perfectly model the true latent posterior distribution; the more we optimize the ELBO, the closer our approximate posterior gets to the true posterior.  Additionally, once trained, the ELBO can be used to estimate the likelihood of observed or generated data as well, since it is learned to approximate the model evidence $\log p(x)$.

## Variational Autoencoders

<Image src="/images/understanding-diffusion-model/vae.png" alt="A Variational Autoencoder graphically represented. Here, encoder $q(z|x)$ defines a distribution over latent variables $z$ for observations $x$, and $p(x|z)$ decodes latent variables into observations." width={211} height={200} className="mx-auto"/>
Visualizing a vanilla Variational Autoencoder. A latent encoder and decoder are learned jointly through the reparameterization trick

In the default formulation of the Variational Autoencoder (VAE) [Kingma and Welling, 2013](https://arxiv.org/abs/1312.6114), we directly maximize the ELBO. This approach is *variational* because we optimize for the best $q_{\bm{\phi}}(z|x)$ amongst a family of potential posterior distributions parameterized by $\bm{\phi}$. It is called an *autoencoder* because it is reminiscent of a traditional autoencoder model, where input data is trained to predict itself after undergoing an intermediate bottlenecking representation step. To make this connection explicit, let us dissect the ELBO term further:

$$
\begin{align}
\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right]
&= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z})p(\boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right]\\
&= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z})\right] + \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\right]\\
&= \underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z})\right]}_\text{reconstruction term} - \underbrace{\mathcal{D}_{\text{KL}}(q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x}) \mid\mid p(\boldsymbol{z}))}_\text{prior matching term}
\end{align}
$$

In this case, we learn an intermediate bottlenecking distribution $q_{\bm{\phi}}(z|x)$ that can be treated as an *encoder*; it transforms inputs into a distribution over possible latents. Simultaneously, we learn a deterministic function $p_{\bm{\theta}}(x|z)$ to convert a given latent vector $z$ into an observation $x$, which can be interpreted as a *decoder*.

The two terms in Equation 19 each have intuitive descriptions: the first term measures the reconstruction likelihood of the decoder from our variational distribution; this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from. The second term measures how similar the learned variational distribution is to a prior belief held over latent variables. Minimizing this term encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function. Maximizing the ELBO is thus equivalent to maximizing its first term and minimizing its second term.

A defining feature of the VAE is how the ELBO is optimized jointly over parameters $\bm{\phi}$ and $\bm{\theta}$. The encoder of the VAE is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior is often selected to be a standard multivariate Gaussian:

$$
\begin{align}
    q_{\bm{\phi}}(z|x) &= \mathcal{N}(z; \bm{\mu}_{\bm{\phi}}(x), \bm{\sigma}_{\bm{\phi}}^2(x)\textbf{I})\\
    p(z) &= \mathcal{N}(z; \bm{0}, \textbf{I})
\end{align}
$$


Then, the KL divergence term of the ELBO can be computed analytically, and the reconstruction term can be approximated using a Monte Carlo estimate.  Our objective can then be rewritten as:

$$
\begin{align}
& \quad \,\underset{\boldsymbol{\phi}, \boldsymbol{\theta}}{\arg\max}\, \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z})\right] - \mathcal{D}_{\text{KL}}(q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x}) \mid\mid p(\boldsymbol{z})) \nonumber \\
& \approx \underset{\boldsymbol{\phi}, \boldsymbol{\theta}}{\arg\max}\, \sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z}^{(l)}) - \mathcal{D}_{\text{KL}}(q_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x}) \mid\mid p(\boldsymbol{z}))
\end{align}
$$


where latents $\{z^{(l)}\}_{l=1}^L$ are sampled from $q_{\bm{\phi}}(z|x)$, for every observation $x$ in the dataset.  However, a problem arises in this default setup: each $z^{(l)}$ that our loss is computed on is generated by a stochastic sampling procedure, which is generally non-differentiable.  Fortunately, this can be addressed via the *reparameterization trick* when $q_{\bm{\phi}}(z|x)$ is designed to model certain distributions, including the multivariate Gaussian.

The reparameterization trick rewrites a random variable as a deterministic function of a noise variable; this allows for the optimization of the non-stochastic terms through gradient descent.  For example, samples from a normal distribution $x \sim \mathcal{N}(x;\mu, \sigma^2)$ with arbitrary mean $\mu$ and variance $\sigma^2$ can be rewritten as:

$$
\begin{align*}
    x &= \mu + \sigma\epsilon \quad \text{with } \epsilon \sim \mathcal{N}(\epsilon; 0, \text{I})
\end{align*}
$$

In other words, arbitrary Gaussian distributions can be interpreted as standard Gaussians (of which $\epsilon$ is a sample) that have their mean shifted from zero to the target mean $\mu$ by addition, and their variance stretched by the target variance $\sigma^2$.  Therefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling the result by the target standard deviation, and shifting it by the target mean.

In a VAE, each $z$ is thus computed as a deterministic function of input $x$ and auxiliary noise variable $\bm{\epsilon}$:

$$
\begin{align*}
    z &= \bm{\mu}_{\bm{\phi}}(x) + \bm{\sigma}_{\bm{\phi}}(x)\odot\bm{\epsilon} \quad \text{with } \bm{\epsilon} \sim \mathcal{N}(\bm{\epsilon};\bm{0}, \textbf{I})
\end{align*}
$$


where $\odot$ represents an element-wise product.  Under this reparameterized version of $z$, gradients can then be computed with respect to $\bm{\phi}$ as desired, to optimize $\bm{\mu}_{\bm{\phi}}$ and $\bm{\sigma}_{\bm{\phi}}$.  The VAE therefore utilizes the reparameterization trick and Monte Carlo estimates to optimize the ELBO jointly over $\bm{\phi}$ and $\bm{\theta}$.

After training a VAE, generating new data can be performed by sampling directly from the latent space $p(z)$ and then running it through the decoder.  Variational Autoencoders are particularly interesting when the dimensionality of $z$ is less than that of input $x$, as we might then be learning compact, useful representations.  Furthermore, when a semantically meaningful latent space is learned, latent vectors can be edited before being passed to the decoder to more precisely control the data generated.

## Hierarchical Variational Autoencoders

A Hierarchical Variational Autoencoder [HVAE]() is a generalization of a VAE that extends to multiple hierarchies over latent variables.  Under this formulation, latent variables themselves are interpreted as generated from other higher-level, more abstract latents. Intuitively, just as we treat our three-dimensional observed objects as generated from a higher-level abstract latent, the people in Plato's cave treat three-dimensional objects as latents that generate their two-dimensional observations.  Therefore, from the perspective of Plato's cave dwellers, their observations can be treated as modeled by a latent hierarchy of depth two (or more).

<Image src="/images/understanding-diffusion-model/hvae.png" alt="A Markovian Hierarchical Variational Autoencoder with $T$ hierarchical latents.  The generative process is modeled as a Markov chain, where each latent $z_t$ is generated only from the previous latent $z_{t+1}$." width={563} height={200} className="mx-auto"/>


Whereas in the general HVAE with $T$ hierarchical levels, each latent is allowed to condition on all previous latents, in this work we focus on a special case which we call a Markovian HVAE (MHVAE).  In a MHVAE, the generative process is a Markov chain; that is, each transition down the hierarchy is Markovian, where decoding each latent $z_t$ only conditions on previous latent $z_{t+1}$.  Intuitively, and visually, this can be seen as simply stacking VAEs on top of each other, as depicted in Figure; another appropriate term describing this model is a Recursive VAE.  Mathematically, we represent the joint distribution and the posterior of a Markovian HVAE as:

$$
\begin{align}
    p(x, z_{1:T}) &= p(z_T)p_{\bm{\theta}}(x|z_1)\prod_{t=2}^{T}p_{\bm{\theta}}(z_{t-1}|z_{t})\\
    q_{\bm{\phi}}(z_{1:T}|x) &= q_{\bm{\phi}}(z_1|x)\prod_{t=2}^{T}q_{\bm{\phi}}(z_{t}|z_{t-1})
\end{align}
$$

Then, we can easily extend the ELBO to be:

$$
\begin{align}
\log p(x) &= \log \int p(x, z_{1:T}) dz_{1:T}         && \text{(Apply Equation 1)}  \\
&= \log \int \frac{p(x, z_{1:T})q_{\bm{\phi}}(z_{1:T}|x)}{q_{\bm{\phi}}(z_{1:T}|x)} dz_{1:T}         && \text{(Multiply by 1 = $\frac{q_{\bm{\phi}}(z_{1:T}|x)}{q_{\bm{\phi}}(z_{1:T}|x)}$)}  \\
&= \log \mathbb{E}_{q_{\bm{\phi}}(z_{1:T}|x)}\left[\frac{p(x, z_{1:T})}{q_{\bm{\phi}}(z_{1:T}|x)}\right]         && \text{(Definition of Expectation)}  \\
&\geq \mathbb{E}_{q_{\bm{\phi}}(z_{1:T}|x)}\left[\log \frac{p(x, z_{1:T})}{q_{\bm{\phi}}(z_{1:T}|x)}\right]         && \text{(Apply Jensen's Inequality)}
\end{align}
$$

We can then plug our joint distribution (Equation 20) and posterior (Equation 21) into Equation 25 to produce an alternate form:


$$
\begin{align}
\mathbb{E}_{q_{\bm{\phi}}(z_{1:T}|x)}\left[\log \frac{p(x, z_{1:T})}{q_{\bm{\phi}}(z_{1:T}|x)}\right]
&= \mathbb{E}_{q_{\bm{\phi}}(z_{1:T}|x)}\left[\log \frac{p(z_T)p_{\bm{\theta}}(x|z_1)\prod_{t=2}^{T}p_{\bm{\theta}}(z_{t-1}|z_{t})}{q_{\bm{\phi}}(z_1|x)\prod_{t=2}^{T}q_{\bm{\phi}}(z_{t}|z_{t-1})}\right]
\end{align}
$$

As we will show below, when we investigate Variational Diffusion Models, this objective can be further decomposed into interpretable components.


## Variational Diffusion Models

The easiest way to think of a Variational Diffusion Model [VDM]() is simply as a Markovian Hierarchical Variational Autoencoder with three key restrictions:

- The latent dimension is exactly equal to the data dimension.
- The structure of the latent encoder at each timestep is not learned; it is pre-defined as a linear Gaussian model. In other words, it is a Gaussian distribution centered around the output of the previous timestep.
- The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at the final timestep \( T \) is a standard Gaussian.

Furthermore, we explicitly maintain the Markov property between hierarchical transitions from a standard Markovian Hierarchical Variational Autoencoder.

Let us expand on the implications of these assumptions. From the first restriction, with some abuse of notation, we can now represent both true data samples and latent variables as $$x_t$$, where $$t=0$$ represents true data samples and $$t \in [1, T]$$ represents a corresponding latent with hierarchy indexed by $$t$$. The VDM posterior is the same as the MHVAE posterior (Equation 21), but can now be rewritten as:

$$
\begin{align}
    q(x_{1:T}|x_0) = \prod_{t = 1}^{T}q(x_{t}|x_{t-1})
\end{align}
$$


From the second assumption, we know that the distribution of each latent variable in the encoder is a Gaussian centered around its previous hierarchical latent.  Unlike a Markovian HVAE, the structure of the encoder at each timestep $t$ is not learned; it is fixed as a linear Gaussian model, where the mean and standard deviation can be set beforehand as hyperparameters [ho2020denoising](), or learned as parameters [kingma2021variational]().  We parameterize the Gaussian encoder with mean $\bm{\mu}_t(x_t) = \sqrt{\alpha_t} x_{t-1}$, and variance $\bm{\Sigma}_t(x_t) = (1 - \alpha_t) \textbf{I}$, where the form of the coefficients are chosen such that the variance of the latent variables stay at a similar scale; in other words, the encoding process is *variance-preserving*.  Note that alternate Gaussian parameterizations are allowed, and lead to similar derivations.  The main takeaway is that $\alpha_t$ is a (potentially learnable) coefficient that can vary with the hierarchical depth $t$, for flexibility.  Mathematically, encoder transitions are denoted as:

$$
\begin{align}
    q(x_{t}|x_{t-1}) = \mathcal{N}(x_{t} ; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) \textbf{I})
\end{align}
$$


From the third assumption, we know that $\alpha_t$ evolves over time according to a fixed or learnable schedule structured such that the distribution of the final latent $p(x_T)$ is a standard Gaussian.  We can then update the joint distribution of a Markovian HVAE (Equation 20) to write the joint distribution for a VDM as: 

$$
\begin{align}
p(x_{0:T}) &= p(x_T)\prod_{t=1}^{T}p_{\bm{\theta}}(x_{t-1}|x_t) \\
\text{where,}&\nonumber\\
p(x_T) &= \mathcal{N}(x_T; \bm{0}, \textbf{I})
\end{align}
$$



Collectively, what this set of assumptions describes is a steady noisification of an image input over time; we progressively corrupt an image by adding Gaussian noise until eventually it becomes completely identical to pure Gaussian noise.  Visually, this process is depicted in Figure [vdm]().


<Image src="/images/understanding-diffusion-model/vdm_base.png" alt="." width={704} height={276} className="mx-auto"/>
A visual representation of a Variational Diffusion Model; $x_0$ represents true data observations such as natural images, $x_T$ represents pure Gaussian noise, and $x_t$ is an intermediate noisy version of $x_0$.  Each $q(x_t|x_{t-1})$ is modeled as a Gaussian distribution that uses the output of the previous state as its mean.

Note that our encoder distributions $q(x_t|x_{t-1})$ are no longer parameterized by $\bm{\phi}$, as they are completely modeled as Gaussians with defined mean and variance parameters at each timestep.  Therefore, in a VDM, we are only interested in learning conditionals $p_{\bm{\theta}}(x_{t-1}|x_{t})$, so that we can simulate new data.  After optimizing the VDM, the sampling procedure is as simple as sampling Gaussian noise from $p(x_T)$ and iteratively running the denoising transitions $p_{\bm{\theta}}(x_{t-1}|x_{t})$ for $T$ steps to generate a novel $x_0$.

Like any HVAE, the VDM can be optimized by maximizing the ELBO, which can be derived as:


$$
\begin{align}
\log p(\boldsymbol{x})
&\geq \mathbb{E}_{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_0)}\right]\\
&=  \begin{aligned}[t]
      \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{1}\mid\boldsymbol{x}_0)}\left[\log p_{\theta}(\boldsymbol{x}_0\mid\boldsymbol{x}_1)\right]}_\text{reconstruction term} &- \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{T-1}\mid\boldsymbol{x}_0)}\left[\mathcal{D}_{\text{KL}}(q(\boldsymbol{x}_T\mid\boldsymbol{x}_{T-1}) \mid\mid p(\boldsymbol{x}_T))\right]}_\text{prior matching term} \\
      &- \sum_{t=1}^{T-1}\underbrace{\mathbb{E}_{q(\boldsymbol{x}_{t-1}, \boldsymbol{x}_{t+1}\mid\boldsymbol{x}_0)}\left[\mathcal{D}_{\text{KL}}(q(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1}) \mid\mid p_{\theta}(\boldsymbol{x}_{t}\mid\boldsymbol{x}_{t+1}))\right]}_\text{consistency term}
    \end{aligned}
\end{align}
$$


$$
\begin{align*}
{\log p(x)}
&= {\log \int p(x_{0:T}) dx_{1:T}}  \\
&= {\log \int \frac{p(x_{0:T})q(x_{1:T}|x_0)}{q(x_{1:T}|x_0)} dx_{1:T}}  \\
&= {\log \mathbb{E}_{q(x_{1:T}|x_0)}\left[\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]}  \\
&\geq {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]} \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)\prod_{t=1}^{T}p_{\bm{\theta}}(x_{t-1}|x_t)}{\prod_{t = 1}^{T}q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)\prod_{t=2}^{T}p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_T|x_{T-1})\prod_{t = 1}^{T-1}q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)\prod_{t=1}^{T-1}p_{\bm{\theta}}(x_{t}|x_{t+1})}{q(x_T|x_{T-1})\prod_{t = 1}^{T-1}q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)}{q(x_T|x_{T-1})}\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \prod_{t = 1}^{T-1}\frac{p_{\bm{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)}{q(x_T|x_{T-1})}\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[ \sum_{t=1}^{T-1} \log \frac{p_{\bm{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)}{q(x_T|x_{T-1})}\right] + \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_0)}\left[ \log \frac{p_{\bm{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{T-1}, x_T|x_0)}\left[\log \frac{p(x_T)}{q(x_T|x_{T-1})}\right] + \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1}, x_t, x_{t+1}|x_0)}\left[\log \frac{p_{\bm{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}  \\
&=  

\begin{aligned}[t]
      {\underbrace{\mathbb{E}_{q(x_{1}|x_0)}\left[\log p_{\theta}(x_0|x_1)\right]}_\text{reconstruction term}} &- {\underbrace{\mathbb{E}_{q(x_{T-1}|x_0)}\left[\mathcal{D}_{\text{KL}} ({q(x_T|x_{T-1})} \mid \mid {p(x_T)}) \right]}_\text{prior matching term}} \\
      &- {\sum_{t=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)}\left[ \mathcal{D}_{\text{KL}} ({q(x_{t}|x_{t-1})} \mid \mid {p_{\theta}(x_{t}|x_{t+1})}) \right]}_\text{consistency term}}
    \end{aligned} 
\end{align*}
$$

The derived form of the ELBO can be interpreted in terms of its individual components:

<Image src="/images/understanding-diffusion-model/first_derivation.png" alt="First ELBO Derivation for a Diffusion Model" width={704} height={276} className="mx-auto"/>
Under our first derivation, a VDM can be optimized by ensuring that for every intermediate $x_t$, the posterior from the latent above it $p_{\bm{\theta}}(x_t|x_{t+1})$ matches the Gaussian corruption of the latent before it $q(x_t|x_{t-1})$.  In this figure, for each intermediate $x_t$, we minimize the difference between the distributions represented by the pink and green arrows.


* $\mathbb{E}_{q(x_{1}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1) \right]$ can be interpreted as a *reconstruction term*, predicting the log probability of the original data sample given the first-step latent.  This term also appears in a vanilla VAE, and can be trained similarly.
* $\mathbb{E}_{q(x_{T-1}|x_0)}\left[ \mathcal{D}_{\text{KL}} ({q(x_T|x_{T-1})}{p(x_T)}) \right]$ is a *prior matching term*; it is minimized when the final latent distribution matches the Gaussian prior.  This term requires no optimization, as it has no trainable parameters; furthermore, as we have assumed a large enough $T$ such that the final distribution is Gaussian, this term effectively becomes zero.
* $\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)}\left[\mathcal{D}_{\text{KL}} ({q(x_{t}|x_{t-1})}{p_{\bm{\theta}}(x_{t}|x_{t+1})}) \right]$ is a *consistency term*; it endeavors to make the distribution at $x_t$ consistent, from both forward and backward processes.  That is, a denoising step from a noisier image should match the corresponding noising step from a cleaner image, for every intermediate timestep; this is reflected mathematically by the KL Divergence.  This term is minimized when we train $p_{\theta}(x_t|x_{t+1})$ to match the Gaussian distribution $q(x_t|x_{t-1})$, which is defined in Equation 27.


Visually, this interpretation of the ELBO is depicted in Figure first_deriv.  The cost of optimizing a VDM is primarily dominated by the third term, since we must optimize over all timesteps $t$.

Under this derivation, all terms of the ELBO are computed as expectations, and can therefore be approximated using Monte Carlo estimates.  However, actually optimizing the ELBO using the terms we just derived might be suboptimal; because the consistency term is computed as an expectation over two random variables $\left\{x_{t-1}, x_{t+1}\right\}$ for every timestep, the variance of its Monte Carlo estimate could potentially be higher than a term that is estimated using only one random variable per timestep.  As it is computed by summing up $T-1$ consistency terms, the final estimated value of the ELBO may have high variance for large $T$ values.

Let us instead try to derive a form for our ELBO where each term is computed as an expectation over only one random variable at a time.  The key insight is that we can rewrite encoder transitions as $q(x_t|x_{t-1}) = q(x_t|x_{t-1}, x_0)$, where the extra conditioning term is superfluous due to the Markov property.  Then, according to Bayes rule, we can rewrite each transition as: 

$$
\begin{align}
q(x_t | x_{t-1}, x_0) = \frac{q(x_{t-1}|x_t, x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}
\end{align}
$$

Armed with this new equation, we can retry the derivation resuming from the ELBO in Equation 34:

$$
\begin{align}
\log p(\boldsymbol{x})
&\geq \mathbb{E}_{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_0)}\right]\\
&=  \begin{aligned}[t]
      \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{1}\mid\boldsymbol{x}_0)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}_0\mid\boldsymbol{x}_1)\right]}_\text{reconstruction term} &- \underbrace{\mathcal{D}_{\text{KL}}(q(\boldsymbol{x}_T\mid\boldsymbol{x}_0) \mid\mid p(\boldsymbol{x}_T))}_\text{prior matching term} \\
      &- \sum_{t=2}^{T} \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{t}\mid\boldsymbol{x}_0)}\left[\mathcal{D}_{\text{KL}}(q(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0) \mid\mid p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t))\right]}_\text{denoising matching term}
    \end{aligned}
\end{align}
$$


$$
\begin{align*}
{\log p(x)}
&\geq {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)\prod_{t=1}^{T}p_{\bm{\theta}}(x_{t-1}|x_t)}{\prod_{t = 1}^{T}q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)\prod_{t=2}^{T}p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_1|x_0)\prod_{t = 2}^{T}q(x_{t}|x_{t-1})}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)\prod_{t=2}^{T}p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_1|x_0)\prod_{t = 2}^{T}q(x_{t}|x_{t-1}, x_0)}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p_{\bm{\theta}}(x_T)p_{\bm{\theta}}(x_0|x_1)}{q(x_1|x_0)} + \log \prod_{t=2}^{T}\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_{t}|x_{t-1}, x_0)}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)}{q(x_1|x_0)} + \log \prod_{t=2}^{T}\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{\frac{q(x_{t-1}|x_{t}, x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)}{q(x_1|x_0)} + \log \prod_{t=2}^{T}\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{\frac{q(x_{t-1}|x_{t}, x_0)\cancel{q(x_t|x_0)}}{\cancel{q(x_{t-1}|x_0)}}}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)}{\cancel{q(x_1|x_0)}} + \log \frac{\cancel{q(x_1|x_0)}}{q(x_T|x_0)} + \log \prod_{t=2}^{T}\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)p_{\bm{\theta}}(x_0|x_1)}{q(x_T|x_0)} +  \sum_{t=2}^{T}\log\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right]}  \\
&= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log \frac{p(x_T)}{q(x_T|x_0)}\right] + \sum_{t=2}^{T}\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right]}  \\
&= {\mathbb{E}_{q(x_{1}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{T}|x_0)}\left[\log \frac{p(x_T)}{q(x_T|x_0)}\right] + \sum_{t=2}^{T}\mathbb{E}_{q(x_{t}, x_{t-1}|x_0)}\left[\log\frac{p_{\bm{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right]}  \\
&= {\underbrace{\mathbb{E}_{q(x_{1}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right]}_\text{reconstruction term} - \underbrace{ \mathcal{D}_{\text{KL}} ({q(x_T|x_0)}{p(x_T)})}_\text{prior matching term} - \sum_{t=2}^{T} \underbrace{\mathbb{E}_{q(x_{t}|x_0)}\left[ \mathcal{D}_{\text{KL}} ({q(x_{t-1}|x_t, x_0)}{p_{\bm{\theta}}(x_{t-1}|x_t)}) \right]}_\text{denoising matching term}} 
\end{align*}
$$

We have therefore successfully derived an interpretation for the ELBO that can be estimated with lower variance, as each term is computed as an expectation of at most one random variable at a time.  This formulation also has an elegant interpretation, which is revealed when inspecting each individual term:

* $\mathbb{E}_{q(x_{1}|x_0)}\left[\log p_{\bm{\theta}}(x_0|x_1)\right]$ can be interpreted as a reconstruction term; like its analogue in the ELBO of a vanilla VAE, this term can be approximated and optimized using a Monte Carlo estimate.
* $\mathcal{D}_{\text{KL}} ({q(x_T|x_0)}{p(x_T)})$ represents how close the distribution of the final noisified input is to the standard Gaussian prior.  It has no trainable parameters, and is also equal to zero under our assumptions.
* $\mathbb{E}_{q(x_{t}|x_0)}\left[ \mathcal{D}_{\text{KL}} ({q(x_{t-1}|x_t, x_0)}{p_{\bm{\theta}}(x_{t-1}|x_t)}) \right]$ is a *denoising matching term*.  We learn desired denoising transition step $p_{\bm{\theta}}(x_{t-1}|x_t)$ as an approximation to tractable, ground-truth denoising transition step $q(x_{t-1}|x_{t}, x_0)$.  The $q(x_{t-1}|x_{t}, x_0)$ transition step can act as a ground-truth signal, since it defines how to denoise a noisy image $x_t$ with access to what the final, completely denoised image $x_0$ should be.  This term is therefore minimized when the two denoising steps match as closely as possible, as measured by their KL Divergence.

As a side note, one observes that in the process of both ELBO derivations (Equation 45 and Equation 51), only the Markov assumption is used; as a result these formulae will hold true for any arbitrary Markovian HVAE.  Furthermore, when we set $T=1$, both of the ELBO interpretations for a VDM exactly recreate the ELBO equation of a vanilla VAE, as written in Equation 19.

In this derivation of the ELBO, the bulk of the optimization cost once again lies in the summation term, which dominates the reconstruction term.  Whereas each KL Divergence term $\mathcal{D}_{\text{KL}} ({q(x_{t-1}|x_t, x_0)}{p_{\bm{\theta}}(x_{t-1}|x_t)} ) $ is difficult to minimize for arbitrary posteriors in arbitrarily complex Markovian HVAEs due to the added complexity of simultaneously learning the encoder, in a VDM we can leverage the Gaussian transition assumption to make optimization tractable.  By Bayes rule, we have:
$$q(x_{t-1}|x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0)q(x_{t-1}|x_0)}{q(x_{t}|x_0)}$$


<Image src="/images/understanding-diffusion-model/second_derivation.png" alt="First ELBO Derivation for a Diffusion Model" width={704} height={327} className="mx-auto"/>
Depicted is an alternate, lower-variance method to optimize a VDM; we compute the form of ground-truth denoising step $q(x_{t-1}|x_t, x_0)$ using Bayes rule, and minimize its KL Divergence with our approximate denoising step $p_{\bm{\theta}}(x_{t-1}|x_t)$.  This is once again denoted visually by matching the distributions represented by the green arrows with those of the pink arrows.  Artistic liberty is at play here; in the full picture, each pink arrow must also stem from $x_0$, as it is also a conditioning term.

As we already know that $q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = \mathcal{N}(x_{t} ; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t)\textbf{I})$ from our assumption regarding encoder transitions (Equation 27), what remains is deriving for the forms of $q(x_t|x_0)$ and $q(x_{t-1}|x_0)$.  Fortunately, these are also made tractable by utilizing the fact that the encoder transitions of a VDM are linear Gaussian models.  Recall that under the reparameterization trick, samples $x_t \sim q(x_t | x_{t-1})$ can be rewritten as:


$$
\begin{align}
    x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\bm{\epsilon} \quad \text{with } \bm{\epsilon} \sim \mathcal{N}(\bm{\epsilon}; \bm{0}, \textbf{I})
\end{align}
$$


and that similarly, samples $x_{t-1} \sim q(x_{t-1} | x_{t-2})$ can be rewritten as:

$$
\begin{align}
    x_{t-1} = \sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_{t-1}}\bm{\epsilon} \quad \text{with } \bm{\epsilon} \sim \mathcal{N}(\bm{\epsilon}; \bm{0}, \textbf{I})
\end{align}
$$



Then, the form of $q(x_t|x_0)$ can be recursively derived through repeated applications of the reparameterization trick.
Suppose that we have access to 2$T$ random noise variables $\{\boldsymbol{\epsilon}_t^*,\boldsymbol{\epsilon}_t\}_{t=0}^T \stackrel{\text{iid}}{\sim} \mathcal{N}(\boldsymbol{\epsilon}; \boldsymbol{0},\textbf{I})$.  Then, for an arbitrary sample $x_t \sim q(x_t|x_0)$, we can rewrite it as:



$$
\begin{align}
x_t  &= \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\bm{\epsilon}_{t-1}^*\\
&= \sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_{t-1}}\bm{\epsilon}_{t-2}^*\right) + \sqrt{1 - \alpha_t}\bm{\epsilon}_{t-1}^*\\
&= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t - \alpha_t\alpha_{t-1}}\bm{\epsilon}_{t-2}^* + \sqrt{1 - \alpha_t}\bm{\epsilon}_{t-1}^*\\
&= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\sqrt{\alpha_t - \alpha_t\alpha_{t-1}}^2 + \sqrt{1 - \alpha_t}^2}\bm{\epsilon}_{t-2} \\
&= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t - \alpha_t\alpha_{t-1} + 1 - \alpha_t}\bm{\epsilon}_{t-2}  \\
&= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + \sqrt{1 - \alpha_t\alpha_{t-1}}\bm{\epsilon}_{t-2} \\
&= \ldots\\
&= \sqrt{\prod_{i=1}^t\alpha_i}x_0 + \sqrt{1 - \prod_{i=1}^t\alpha_i}\bm{\bm{\epsilon}}_0\\
&= \sqrt{\bar\alpha_t}x_0 + \sqrt{1 - \bar\alpha_t}\bm{\bm{\epsilon}}_0 \\
&\sim \mathcal{N}(x_{t} ; \sqrt{\bar\alpha_t}x_0, \left(1 - \bar\alpha_t\right)\textbf{I}) 
\end{align}
$$



where in Equation 63 we have utilized the fact that the [sum of two independent Gaussian random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables) remains a Gaussian with mean being the sum of the two means, and variance being the sum of the two variances.
Interpreting $\sqrt{1 - \alpha_t}\bm{\epsilon}_{t-1}^*$ as a sample from Gaussian $\mathcal{N}(\bm{0}, (1 - \alpha_t)\textbf{I})$, and $\sqrt{\alpha_t - \alpha_t\alpha_{t-1}}\bm{\epsilon}_{t-2}^*$ as a sample from Gaussian $\mathcal{N}(\bm{0}, (\alpha_t - \alpha_t\alpha_{t-1})\textbf{I})$, we can then treat their sum as a random variable sampled from Gaussian $\mathcal{N}(\bm{0}, (1 - \alpha_t + \alpha_t - \alpha_t\alpha_{t-1})\textbf{I}) = \mathcal{N}(\bm{0}, (1 - \alpha_t\alpha_{t-1})\textbf{I})$.  A sample from this distribution can then be represented using the reparameterization trick as $\sqrt{1 - \alpha_t\alpha_{t-1}}\bm{\epsilon}_{t-2}$, as in Equation 66.

We have therefore derived the Gaussian form of $q(x_t|x_0)$.  This derivation can be modified to also yield the Gaussian parameterization describing $q(x_{t-1}|x_0)$.  Now, knowing the forms of both $q(x_t|x_0)$ and $q(x_{t-1}|x_0)$, we can proceed to calculate the form of $q(x_{t-1}|x_t, x_0)$ by substituting into the Bayes rule expansion:

$$
\begin{align}
q(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0)
&= \frac{q(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0)q(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_0)}{q(\boldsymbol{x}_{t}\mid\boldsymbol{x}_0)}  \\
&= \frac{\mathcal{N}(\boldsymbol{x}_{t} ; \sqrt{\alpha_t} \boldsymbol{x}_{t-1}, (1 - \alpha_t)\textbf{I})\mathcal{N}(\boldsymbol{x}_{t-1} ; \sqrt{\bar\alpha_{t-1}}\boldsymbol{x}_0, (1 - \bar\alpha_{t-1}) \textbf{I})}{\mathcal{N}(\boldsymbol{x}_{t} ; \sqrt{\bar\alpha_{t}}\boldsymbol{x}_0, (1 - \bar\alpha_{t})\textbf{I})}  \\
&\propto \mathcal{N}(\boldsymbol{x}_{t-1} ; \underbrace{\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})\boldsymbol{x}_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\boldsymbol{x}_0}{1 -\bar\alpha_{t}}}_{\mu_q(\boldsymbol{x}_t, \boldsymbol{x}_0)}, \underbrace{\frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}\textbf{I}}_{\boldsymbol{\Sigma}_q(t)})
\end{align}
$$ 

$$
\begin{align*}
{q(x_{t-1}|x_t, x_0)}
&= \frac{q(x_t | x_{t-1}, x_0)q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \\
&= {\frac{\mathcal{N}(x_{t} ; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t)\textbf{I})\mathcal{N}(x_{t-1} ; \sqrt{\bar\alpha_{t-1}}x_0, (1 - \bar\alpha_{t-1}) \textbf{I})}{\mathcal{N}(x_{t} ; \sqrt{\bar\alpha_{t}}x_0, (1 - \bar\alpha_{t})\textbf{I})}}  \\
&\propto {\text{exp}\left\{-\left[\frac{(x_{t} - \sqrt{\alpha_t} x_{t-1})^2}{2(1 - \alpha_t)} + \frac{(x_{t-1} - \sqrt{\bar\alpha_{t-1}} x_0)^2}{2(1 - \bar\alpha_{t-1})} - \frac{(x_{t} - \sqrt{\bar\alpha_t} x_{0})^2}{2(1 - \bar\alpha_t)} \right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left[\frac{(x_{t} - \sqrt{\alpha_t} x_{t-1})^2}{1 - \alpha_t} + \frac{(x_{t-1} - \sqrt{\bar\alpha_{t-1}} x_0)^2}{1 - \bar\alpha_{t-1}} - \frac{(x_{t} - \sqrt{\bar\alpha_t} x_{0})^2}{1 - \bar\alpha_t} \right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left[\frac{(-2\sqrt{\alpha_t} x_{t}x_{t-1} + \alpha_t x_{t-1}^2)}{1 - \alpha_t} + \frac{(x_{t-1}^2 - 2\sqrt{\bar\alpha_{t-1}}x_{t-1} x_0)}{1 - \bar\alpha_{t-1}} + C(x_t, x_0)\right]\right\}} \\
&\propto {\text{exp}\left\{-\frac{1}{2}\left[- \frac{2\sqrt{\alpha_t} x_{t}x_{t-1}}{1 - \alpha_t} + \frac{\alpha_t x_{t-1}^2}{1 - \alpha_t} + \frac{x_{t-1}^2}{1 - \bar\alpha_{t-1}} - \frac{2\sqrt{\bar\alpha_{t-1}}x_{t-1} x_0}{1 - \bar\alpha_{t-1}}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left[(\frac{\alpha_t}{1 - \alpha_t} + \frac{1}{1 - \bar\alpha_{t-1}})x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left[\frac{\alpha_t(1-\bar\alpha_{t-1}) + 1 - \alpha_t}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left[\frac{\alpha_t-\bar\alpha_{t} + 1 - \alpha_t}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left[\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left(\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}\right)\left[x_{t-1}^2 - 2\frac{\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)}{\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}}x_{t-1}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left(\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}\right)\left[x_{t-1}^2 - 2\frac{\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}x_{t-1}\right]\right\}}  \\
&= {\text{exp}\left\{-\frac{1}{2}\left(\frac{1}{\frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}}\right)\left[x_{t-1}^2 - 2\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}x_{t-1}\right]\right\}}  \\
&\propto {\mathcal{N}(x_{t-1} ;} \underbrace{{\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}}_{\mu_q(x_t, x_0)}, \underbrace{{\frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}\textbf{I}}}_{\bm{\Sigma}_q(t)}) 
\end{align*}
$$


where in Equation 73, $C(x_t, x_0)$ is a constant term with respect to $x_{t-1}$ computed as a combination of only $x_t$, $x_0$, and $\alpha$ values; this term is implicitly returned in Equation 78 to complete the square.

We have therefore shown that at each step, $x_{t-1} \sim q(x_{t-1}| x_t, x_0)$ is normally distributed, with mean $\bm{\mu}_q(x_t, x_0)$ that is a function of $x_t$ and $x_0$, and variance $\bm{\Sigma}_q(t)$ as a function of $\alpha$ coefficients.  These $\alpha$ coefficients are known and fixed at each timestep; they are either set permanently when modeled as hyperparameters, or treated as the current inference output of a network that seeks to model them.  Following Equation 78, we can rewrite our variance equation as $\bm{\Sigma}_q(t) = \sigma_q^2(t)\textbf{I}$, where:


$$
\begin{align}
    \sigma_q^2(t) = \frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}} 
\end{align}
$$



In order to match approximate denoising transition step $p_{\bm{\theta}}(x_{t-1}|x_t)$ to ground-truth denoising transition step $q(x_{t-1}| x_t, x_0)$ as closely as possible, we can also model it as a Gaussian.  Furthermore, as all $\alpha$ terms are known to be frozen at each timestep, we can immediately construct the variance of the approximate denoising transition step to also be $\bm{\Sigma}_q(t) = \sigma_q^2(t)\textbf{I}$.  We must parameterize its mean $\bm{\mu}_{\bm{\theta}}(x_t, t)$ as a function of $x_t$, however, since $p_{\bm{\theta}}(x_{t-1}|x_t)$ does not condition on $x_0$.

Recall that the [KL Divergence between two Gaussian distributions](https://en.wikipedia.org/wiki/Kullback\%E2\%80\%93Leibler_divergence#Multivariate_normal_distributions) is:

$$
\begin{align}
    \mathcal{D}_{\text{KL}} ({\mathcal{N}(x; \bm{\mu}_x,\bm{\Sigma}_x)}{\mathcal{N}(y; \bm{\mu}_y,\bm{\Sigma}_y)})
&=\frac{1}{2}\left[\log\frac{|\bm{\Sigma}_y|}{|\bm{\Sigma}_x|} - d + \text{tr}(\bm{\Sigma}_y^{-1}\bm{\Sigma}_x)
+ (\bm{\mu}_y-\bm{\mu}_x)^T \bm{\Sigma}_y^{-1} (\bm{\mu}_y-\bm{\mu}_x)\right]
\end{align}
$$

In our case, where we can set the variances of the two Gaussians to match exactly, optimizing the KL Divergence term reduces to minimizing the difference between the means of the two distributions:

$$
\begin{align}
& \quad \,\underset{\boldsymbol{\theta}}{\arg\min}\,  \mathcal{D}_{\text{KL}}(q(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0) \mid\mid p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t)) \nonumber \\
&= \underset{\boldsymbol{\theta}}{\arg\min}\, \mathcal{D}_{\text{KL}}\left(\mathcal{N}\left(\boldsymbol{x}_{t-1}; \boldsymbol{\mu}_q,\boldsymbol{\Sigma}_q\left(t\right)\right) \mid\mid \mathcal{N}\left(\boldsymbol{x}_{t-1}; \boldsymbol{\mu}_{\boldsymbol{\theta}},\boldsymbol{\Sigma}_q\left(t\right)\right)\right)\\
&=\underset{\boldsymbol{\theta}}{\arg\min}\, \frac{1}{2\sigma_q^2(t)}\left[\left\lVert\boldsymbol{\mu}_{\boldsymbol{\theta}}-\boldsymbol{\mu}_q\right\rVert_2^2\right]
\end{align}
$$

$$
\begin{align*}
& \quad \,\argmin_{\bm{\theta}} \mathcal{D}_{\text{KL}} ({q(x_{t-1}|x_t, x_0)}{p_{\bm{\theta}}(x_{t-1}|x_t)}) \nonumber \\
&= \argmin_{\bm{\theta}} \mathcal{D}_{\text{KL}} ({\mathcal{N}(x_{t-1}; \bm{\mu}_q,\bm{\Sigma}_q(t))}{\mathcal{N}(x_{t-1}; \bm{\mu}_{\bm{\theta}},\bm{\Sigma}_q(t))} ) \\
&=\argmin_{\bm{\theta}}\frac{1}{2}\left[\log\frac{|\bm{\Sigma}_q(t)|}{|\bm{\Sigma}_q(t)|} - d + \text{tr}(\bm{\Sigma}_q(t)^{-1}\bm{\Sigma}_q(t))
+ (\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)^T \bm{\Sigma}_q(t)^{-1} (\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)\right]\\
&=\argmin_{\bm{\theta}}\frac{1}{2}\left[\log1 - d + d + (\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)^T \bm{\Sigma}_q(t)^{-1} (\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)\right]\\
&=\argmin_{\bm{\theta}}\frac{1}{2}\left[(\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)^T \bm{\Sigma}_q(t)^{-1} (\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)\right]\\
&=\argmin_{\bm{\theta}}\frac{1}{2}\left[(\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)^T \left(\sigma_q^2(t)\textbf{I}\right)^{-1} (\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q)\right]\\
&=\argmin_{\bm{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert\bm{\mu}_{\bm{\theta}}-\bm{\mu}_q\right\rVert_2^2\right]
\end{align*}
$$



where we have written $\bm{\mu}_q$ as shorthand for $\bm{\mu}_q(x_t, x_0)$, and $\bm{\mu}_{\bm{\theta}}$ as shorthand for $\bm{\mu}_{\bm{\theta}}(x_t, t)$ for brevity.  In other words, we want to optimize a $\bm{\mu}_{\bm{\theta}}(x_t, t)$ that matches $\bm{\mu}_q(x_t, x_0)$, which from our derived Equation 78, takes the form: 

$$
\begin{align}
    \bm{\mu}_q(x_t, x_0) = \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}
\end{align}
$$

As $\bm{\mu}_{\bm{\theta}}(x_t, t)$ also conditions on $x_t$, we can match $\bm{\mu}_q(x_t, x_0)$ closely by setting it to the following form:

$$
\begin{align}
    \bm{\mu}_{\bm{\theta}}(x_t, t) = \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{\bm{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}
\end{align}
$$


where $\hat x_{\bm{\theta}}(x_t, t)$ is parameterized by a neural network that seeks to predict $x_0$ from noisy image $x_t$ and time index $t$. Then, the optimization problem simplifies to:

$$
\begin{align}
& \quad \,\underset{\boldsymbol{\theta}}{\arg\min}\,  \mathcal{D}_{\text{KL}}(q(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0) \mid\mid p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t)) \nonumber \\
&= \underset{\boldsymbol{\theta}}{\arg\min}\, \mathcal{D}_{\text{KL}}\left(\mathcal{N}\left(\boldsymbol{x}_{t-1}; \boldsymbol{\mu}_q,\boldsymbol{\Sigma}_q\left(t\right)\right) \mid\mid \mathcal{N}\left(\boldsymbol{x}_{t-1}; \boldsymbol{\mu}_{\boldsymbol{\theta}},\boldsymbol{\Sigma}_q\left(t\right)\right)\right)\\
&=\underset{\boldsymbol{\theta}}{\arg\min}\, \frac{1}{2\sigma_q^2(t)}\frac{\bar\alpha_{t-1}(1-\alpha_t)^2}{(1 -\bar\alpha_{t})^2}\left[\left\lVert\hat{\boldsymbol{x}}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) - \boldsymbol{x}_0\right\rVert_2^2\right]
\end{align}
$$


$$
\begin{align*}
&  {\quad \,\argmin_{\bm{\theta}} \mathcal{D}_{\text{KL}} ({q(x_{t-1}|x_t, x_0)}{p_{\bm{\theta}}(x_{t-1}|x_t)})} \nonumber \\
&=  {\argmin_{\bm{\theta}}\mathcal{D}_{\text{KL}} ({\mathcal{N}\left(x_{t-1}; \bm{\mu}_q,\bm{\Sigma}_q\left(t\right)\right)}{\mathcal{N}\left(x_{t-1}; \bm{\mu}_{\bm{\theta}},\bm{\Sigma}_q\left(t\right)\right)} ) }  \\
&= {\argmin_{\bm{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{\bm{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}-\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}\right\rVert_2^2\right]}  \\
&= {\argmin_{\bm{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert\frac{\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{\bm{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}-\frac{\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}\right\rVert_2^2\right]}  \\
&= {\argmin_{\bm{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert\frac{\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)}{1 -\bar\alpha_{t}}\left(\hat x_{\bm{\theta}}(x_t, t) - x_0\right)\right\rVert_2^2\right]}  \\
&= {\argmin_{\bm{\theta}}\frac{1}{2\sigma_q^2(t)}\frac{\bar\alpha_{t-1}(1-\alpha_t)^2}{(1 -\bar\alpha_{t})^2}\left[\left\lVert\hat x_{\bm{\theta}}(x_t, t) - x_0\right\rVert_2^2\right]} 
\end{align*}
$$



Therefore, optimizing a VDM boils down to learning a neural network to predict the original ground truth image from an arbitrarily noisified version of [ho2020denoising]().  Furthermore, minimizing the summation term of our derived ELBO objective (Equation 51) across all noise levels can be approximated by minimizing the expectation over all timesteps:

$$
\begin{align}
\argmin_{\bm{\theta}}\mathbb{E}_{t\sim U\{2, T\}}\left[\mathbb{E}_{q(x_{t}|x_0)}\left[ \mathcal{D}_{\text{KL}} ({q(x_{t-1}|x_t, x_0)}{p_{\bm{\theta}}(x_{t-1}|x_t)}) \right]\right] 
\end{align}
$$

which can then be optimized using stochastic samples over timesteps.